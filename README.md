# DynamicExploitation

Possibly original (yet probably not) generalized game playing AI

## Overview

In this repository I have created what I am labelling as a dynamic exploiter. The model can learn any game that has a definable relationship between state and legal actions, as well as a well defined terminal state. (The exact requirements can be found in 'diy.py', which lists clear instructions on all non-generalizable, game-specific functions that must be updated, *e.g the rules of the game*). 

## Naming

The technique is so named on account of the conventional trade-off between *exploitation* and *exploration* that exists in traditional reinforcement learning models, whereby a model must choose between developing existing competencies and exploring its environment further. However in Dynamic exploitation, the model is somewhat in charge of its own exploitation, and uses convergence data to inform itself on how to converge further.

## Operations and Architecture

The model uses **no libraries** and operates using a combination of reinforcement learning and gradient descent. The model will preform a series of attempts known as a trial. Each trial will be defined by the peramater 'p', which determines the rate of model evolution. It will then preform 3 dimensional gradient descent to optimize the p-value with respect to the rate of convergence. Visualize a piecewise function consisting of consecutive convergence rates corresponding to loss/reward over time, and where each 'piece' is an optimized trial.

## User defined peramaters

Apart from the game specific functions listed in 'diy.py', the user can define a series of peramaters pertaining to the model's training, found in 'settings.py'. The most impactful of which is the T value. The T value denotes the number of sub_trials in each trial. The T value should be as large as possible and should only be limited by the user's hardware. The initial p value P_0 can be somewhat arbitrarily defined if the model functions properly. However, numerous values should be experimented with. The learning Rate LR is imperative to the models success. The most important aspect to its definition is its sign. A negative learning rate implies that the model should be reducing loss, while a positive rate means it should be maximizing reward.


## Future Developments

In the models current form, it optimizes its convergence with respect to **one perameter** only. The generalizability of this technique should be clear in that the model can be extended to optimize other peramaters affecting its convergence. The model opperates by assuming an randomization/exploration rate for successful sub trials and simply uses twice that value for unsuccesful ones. This is an **arbitrary choice**. Ideally the model would take into account two distinguished p values, p_s and p_f, allowing it to dynamically decide how much it should reward success and penalize failure.

